{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d73da81e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      " US slams Iran for attack on Salman Rushdie, terms it  despicable, disgusting   \n",
      "20\n",
      "2\n",
      " FIFA bans India  takes away hosting rights of women s under 17 World Cup  \n",
      "19\n",
      "3\n",
      " Is dyslexia a gift? Here are some surprising benefits  \n",
      "38\n",
      "4\n",
      " Britain s RAF flights transporting ammunition from Pakistan to Ukraine?  \n",
      "21\n",
      "5\n",
      " Laal Singh Chaddha box office day 5 collection  Film might shut shop by next week  earns only â‚¹46 crore so far  \n",
      "19\n",
      "6\n",
      " Kapil Sharma shares pic with his daughter, Anayra Sharma, calls her  my little world   \n",
      "24\n",
      "7\n",
      " Aakanksha Singh  As actors we should readily take up challenges  \n",
      "20\n",
      "8\n",
      " Shah, Nadda meet Bihar BJP core group days after Nitish moves on to RJD  \n",
      "8\n",
      "9\n",
      "  Pandemic far from over   Delhi L G as capital sees rise in Covid 19 deaths  \n",
      "14\n",
      "10\n",
      " Horoscope Today  Astrological prediction for August 16, 2022  \n",
      "83\n"
     ]
    }
   ],
   "source": [
    "from csv import writer\n",
    "import io \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "df = pd.read_excel('Input.xlsx')\n",
    "df1 = df.reset_index()['URL']#selecting only the URL from the excel and saving it in df1\n",
    "df2 = df.reset_index()['URL_ID']\n",
    "#df6 = []\n",
    "#for x in range(0,149):\n",
    " #   df6.append(df1[x])\n",
    "#df6.reverse()\n",
    "headers = requests.utils.default_headers()\n",
    "headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n",
    "})\n",
    "def removal(fname, text):\n",
    "    file = open(fname,\"r\")\n",
    "    f = file.read().split()\n",
    "    final = \"\"\n",
    "    for word in text:\n",
    "        for g in f:\n",
    "            if word == g:\n",
    "                word = word.replace(g,'')\n",
    "        #print(word)\n",
    "        final = final+word+\" \"\n",
    "    final_list = final.split()\n",
    "    return final_list\n",
    "def score(fname,texts):\n",
    "    file = open(fname,\"r\")\n",
    "    f = file.read().split()\n",
    "    count = 0\n",
    "    for i in texts:\n",
    "        for j in f:\n",
    "            if i == j :\n",
    "                count = count +1\n",
    "    file.flush()\n",
    "    file.close()\n",
    "    return count        \n",
    "with open('Output Data Structure.csv','w',encoding='utf-8',newline='') as fn:\n",
    "            thewriter = writer(fn)\n",
    "            header = ['URL_ID','URL','POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']\n",
    "            thewriter.writerow(header)\n",
    "            url_id = 0\n",
    "            for url in df6:    \n",
    "                    url_id = url_id+1\n",
    "                    print(url_id)\n",
    "                    html = requests.get(url,headers=headers)\n",
    "                    links = requests.get(url,headers=headers)\n",
    "                    link = BeautifulSoup(links.content,'html.parser')\n",
    "                    content = link.find_all('div', class_=\"detail\")\n",
    "                    titles = link.find_all('h1',class_='hdg1')\n",
    "                    clean = open(\"cleaning.txt\",\"r\",encoding='utf-8')\n",
    "                    b = clean.read().split()\n",
    "                    clean.close()\n",
    "                    with open('data.txt', 'r+', encoding='utf-8') as f:\n",
    "                        #print(r['body'], file=f)\n",
    "                    #file = open(\"data.txt\",\"w\")\n",
    "                        para = BeautifulSoup(str(content), \"lxml\").text\n",
    "                        title = BeautifulSoup(str(titles), \"lxml\").text\n",
    "                        l = title.split()\n",
    "                        #print(l)\n",
    "                        a = para.split()\n",
    "                        new = \"\"\n",
    "                        for x in l:\n",
    "                            for y in b:\n",
    "                                x= x.replace(y,' ')\n",
    "                            new = new+x+\" \" \n",
    "                        print(new)\n",
    "                        for x in a:\n",
    "                            for y in b:\n",
    "                                x= x.replace(y,' ')\n",
    "                           #print(x)\n",
    "                            new = new+x+\" \"\n",
    "                        j = new\n",
    "                        new = new.upper()\n",
    "                        new = new.strip()\n",
    "                        t = new.split()\n",
    "                        text = new.split()\n",
    "                        list_stopWords = ['StopWords_Auditor.txt','StopWords_Currencies.txt','StopWords_DatesandNumbers.txt','StopWords_Generic.txt','StopWords_GenericLong.txt','StopWords_Geographic.txt','StopWords_Names.txt']\n",
    "                        for i in list_stopWords:\n",
    "                            text = removal(i,text)\n",
    "                        after_cleaning = \"\"\n",
    "                        for i in text:\n",
    "                            after_cleaning = after_cleaning+i+\" \"\n",
    "                        #print(after_cleaning)\n",
    "                        #print(text)\n",
    "                        #print(len(text))\n",
    "                        f.write(after_cleaning)\n",
    "                        #cleaning = '''[]<>/\"'-='''  \n",
    "                        f.flush()\n",
    "                        f.close()\n",
    "                    #print(text)\n",
    "                    positive_score = 0\n",
    "                    negative_score = 0\n",
    "                    with open('data.txt', 'r', encoding='utf-8') as f:\n",
    "                        texts = f.read().lower().split()\n",
    "                        #print(texts)\n",
    "                        words_after_cleaning = len(texts)\n",
    "                        list_data_analysis = ['negative-words.txt','positive-words.txt']\n",
    "                        score_list = [0,0]\n",
    "                        c = 0\n",
    "                        for i in list_data_analysis:\n",
    "                            number = score(i,texts)\n",
    "                            score_list[c] = int (number)\n",
    "                            c = c+1\n",
    "                        #print(score_list)\n",
    "                        #print(words_after_cleaning)\n",
    "                        negative_score = score_list[0]\n",
    "                        positive_score = score_list[1]\n",
    "                        #print(positive_score)\n",
    "                        #print(texts)\n",
    "                    polarity_score = (score_list[1]-score_list[0])/((score_list[1]+score_list[0])+0.000001)\n",
    "                    subjectivity_score = (score_list[1]+score_list[0])/ ((words_after_cleaning) + 0.000001)\n",
    "                    number_of_words = len(t)\n",
    "                    number_of_sentences = sent_tokenize(new)\n",
    "                    sentence_count = len(number_of_sentences)\n",
    "                    print(sentence_count)\n",
    "                    Average_Sentence_Length = number_of_words/sentence_count\n",
    "                    complex_words = 0\n",
    "                    for words in t:\n",
    "                        length = len(words)\n",
    "                        if length > 2:\n",
    "                            complex_words = complex_words+1\n",
    "                    Percentage_of_Complex_words = complex_words/number_of_words\n",
    "                    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
    "                    AVG_NUMBER_OF_WORDS_PER_SENTENCE = Average_Sentence_Length \n",
    "                    stop_words = set(stopwords.words('english')) \n",
    "                    s = [x.upper() for x in stop_words]\n",
    "                    #print(s)\n",
    "                    new_s = \"\"\n",
    "                    for r in t: \n",
    "                        for i in s: \n",
    "                            if i == r:\n",
    "                                check = 0\n",
    "                                break\n",
    "                            else:\n",
    "                                check = 1\n",
    "                        if check == 1:\n",
    "                            new_s = new_s+r+\" \"\n",
    "                    punctuation_list = ['!','.','?',',']\n",
    "                    new_filter = \"\"\n",
    "                    for char in new_s:\n",
    "                        for i in punctuation_list:\n",
    "                            if char == i:\n",
    "                                char = char.replace(i,'')\n",
    "                        new_filter = new_filter+char\n",
    "                    word_count = len(new_filter.split())\n",
    "                    vowel_list = ['A','E','I','O','U']\n",
    "                    Syllable_Count_Per_Word = 0\n",
    "                    for word in t:\n",
    "                        for char in word:\n",
    "                            for i in vowel_list:\n",
    "                                if char == i:\n",
    "                                    Syllable_Count_Per_Word = Syllable_Count_Per_Word + 1\n",
    "                        if word.endswith(\"ED\") or word.endswith(\"ES\"):\n",
    "                            Syllable_Count_Per_Word = Syllable_Count_Per_Word - 1\n",
    "                    Personal_pronoun_count = ['I', 'WE', 'MY','OURS','US']\n",
    "                    Personal_pronoun = 0\n",
    "                    for word in t:\n",
    "                        for i in Personal_pronoun_count:\n",
    "                            if word == i:\n",
    "                                Personal_pronoun = Personal_pronoun + 1\n",
    "                    j_count = 0\n",
    "                    check_j = j.split()\n",
    "                    for word in check_j:\n",
    "                        if word == 'US':\n",
    "                            j_count = j_count+1\n",
    "                    Personal_pronoun = Personal_pronoun - j_count\n",
    "                    blank_count = 0\n",
    "                    for char in new:\n",
    "                        if char == ' ':\n",
    "                            blank_count = blank_count +1\n",
    "                    total_characters = len(new) - blank_count\n",
    "                    total_characters\n",
    "                    Average_Word_Length = total_characters/number_of_words\n",
    "                    info = [url_id,url,positive_score,negative_score,polarity_score,subjectivity_score,Average_Sentence_Length,Percentage_of_Complex_words,Fog_Index,AVG_NUMBER_OF_WORDS_PER_SENTENCE,complex_words,word_count,Syllable_Count_Per_Word,Personal_pronoun,Average_Word_Length]\n",
    "                    thewriter.writerow(info)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e618ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6f594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
